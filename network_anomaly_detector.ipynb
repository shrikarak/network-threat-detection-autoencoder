{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proactive Network Threat Detection with a Deep Learning Autoencoder\n",
    "\n",
    "**Copyright (c) 2026 Shrikara Kaudambady. All rights reserved.**\n",
    "\n",
    "This notebook implements an advanced threat detection system using an **Autoencoder**, a type of neural network. The model is trained on 'normal' network traffic to learn its underlying patterns. It then identifies potential threats by flagging traffic that it cannot reconstruct accurately, indicating a deviation from the norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Simulation\n",
    "\n",
    "We generate a synthetic dataset of network connections. Most of the data represents normal traffic, but we will inject specific anomalies like a port scan and data exfiltration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n_samples = 5000\n",
    "\n",
    "# Generate Normal Data\n",
    "normal_data = pd.DataFrame({\n",
    "    'duration': np.random.uniform(0, 5, n_samples),\n",
    "    'protocol_type': np.random.choice(['tcp', 'udp'], n_samples, p=[0.8, 0.2]),\n",
    "    'service': np.random.choice(['http', 'smtp', 'private'], n_samples, p=[0.7, 0.1, 0.2]),\n",
    "    'src_bytes': np.random.randint(100, 5000, n_samples),\n",
    "    'dst_bytes': np.random.randint(200, 15000, n_samples)\n",
    "})\n",
    "normal_data['is_anomaly'] = 0\n",
    "\n",
    "# Anomaly 1: Port Scan\n",
    "port_scan_data = pd.DataFrame({\n",
    "    'duration': np.random.uniform(0, 0.1, 50),\n",
    "    'protocol_type': 'tcp',\n",
    "    'service': np.random.choice(['telnet', 'gopher', 'shell', 'login', 'finger'], 50),\n",
    "    'src_bytes': 0,\n",
    "    'dst_bytes': 0\n",
    "})\n",
    "port_scan_data['is_anomaly'] = 1\n",
    "\n",
    "# Anomaly 2: Data Exfiltration\n",
    "exfil_data = pd.DataFrame({\n",
    "    'duration': np.random.uniform(10, 20, 10),\n",
    "    'protocol_type': 'tcp',\n",
    "    'service': 'private',\n",
    "    'src_bytes': np.random.randint(1_000_000, 5_000_000, 10), # Unusually high source bytes\n",
    "    'dst_bytes': np.random.randint(100, 500, 10)\n",
    "})\n",
    "exfil_data['is_anomaly'] = 1\n",
    "\n",
    "df = pd.concat([normal_data, port_scan_data, exfil_data], ignore_index=True)\n",
    "df = df.sample(frac=1).reset_index(drop=True) # Shuffle\n",
    "\n",
    "print(f\"Generated dataset with {len(df)} records.\")\n",
    "print(f\"Number of anomalies: {df['is_anomaly'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Feature Engineering and Preprocessing\n",
    "We convert categorical data to numbers (one-hot encoding) and scale all features to a [0, 1] range, which is critical for training neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encode categorical features\n",
    "df_processed = pd.get_dummies(df, columns=['protocol_type', 'service'], dummy_na=False)\n",
    "\n",
    "# Separate original labels before dropping them for training\n",
    "labels = df_processed['is_anomaly']\n",
    "df_processed = df_processed.drop('is_anomaly', axis=1)\n",
    "\n",
    "# Scale all features to the [0, 1] range\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(df_processed)\n",
    "\n",
    "# Split data: train ONLY on normal data, test on the full mixed dataset\n",
    "X_train_normal = X_scaled[labels == 0]\n",
    "X_test_mixed = X_scaled\n",
    "y_test_labels = labels\n",
    "\n",
    "print(f\"Training data shape (normal traffic only): {X_train_normal.shape}\")\n",
    "print(f\"Test data shape (mixed traffic): {X_test_mixed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Build and Train the Autoencoder Model\n",
    "The model has an encoder that compresses the data and a decoder that reconstructs it. It is trained to minimize the reconstruction error on normal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train_normal.shape[1]\n",
    "encoding_dim = 8\n",
    "\n",
    "input_layer = layers.Input(shape=(input_dim,))\n",
    "# Encoder\n",
    "encoder = layers.Dense(32, activation='relu')(input_layer)\n",
    "encoder = layers.Dense(16, activation='relu')(encoder)\n",
    "encoder = layers.Dense(encoding_dim, activation='relu')(encoder)\n",
    "# Decoder\n",
    "decoder = layers.Dense(16, activation='relu')(encoder)\n",
    "decoder = layers.Dense(32, activation='relu')(decoder)\n",
    "decoder = layers.Dense(input_dim, activation='sigmoid')(decoder)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "autoencoder.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "autoencoder.summary()\n",
    "\n",
    "# Train the model on normal data ONLY\n",
    "history = autoencoder.fit(\n",
    "    X_train_normal, X_train_normal, \n",
    "    epochs=20, \n",
    "    batch_size=32, \n",
    "    shuffle=True, \n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Anomaly Detection and Visualization\n",
    "We calculate the reconstruction error for all data points. A high error indicates an anomaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model's predictions on the full test set\n",
    "predictions = autoencoder.predict(X_test_mixed)\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE) as the reconstruction error\n",
    "mae = np.mean(np.abs(X_test_mixed - predictions), axis=1)\n",
    "df['reconstruction_error'] = mae\n",
    "\n",
    "# Plot the distribution of errors\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(df[df['is_anomaly'] == 0]['reconstruction_error'], bins=50, label='Normal', color='blue', kde=True)\n",
    "sns.histplot(df[df['is_anomaly'] == 1]['reconstruction_error'], bins=50, label='Anomaly', color='red', kde=True)\n",
    "plt.title('Distribution of Reconstruction Errors')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Set a threshold for anomaly detection (e.g., 95th percentile of normal errors)\n",
    "threshold = np.percentile(df[df['is_anomaly'] == 0]['reconstruction_error'], 95)\n",
    "print(f\"\\nAnomaly detection threshold set at: {threshold:.4f}\")\n",
    "\n",
    "# Identify anomalies\n",
    "detected_anomalies = df[df['reconstruction_error'] > threshold]\n",
    "\n",
    "print(f\"\\nFound {len(detected_anomalies)} potential threats.\")\n",
    "print(\"--- Detected Anomalies --- \")\n",
    "display(detected_anomalies.head(20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}